# K-최근접 이웃 (K-Nearest Neighbors, KNN)

## 소개
K-최근접 이웃(KNN) 알고리즘은 지도 학습의 일종으로, 분류와 회귀 문제에 모두 사용할 수 있는 간단하고 직관적인 방법입니다. KNN은 새로운 데이터 포인트의 레이블을 예측하기 위해 가장 가까운 K개의 이웃 데이터를 참조합니다.

## 작동 원리
KNN 알고리즘의 작동 원리는 다음과 같습니다:
1. **거리 계산**: 새로운 데이터 포인트와 기존 데이터 포인트들 간의 거리를 계산합니다. 일반적으로 유클리드 거리(Euclidean distance)를 사용하지만, Manhattan distance나 Minkowski distance 등 다른 거리 측정 방법도 사용할 수 있습니다.
2. **이웃 선택**: 계산된 거리 값을 기준으로 가장 가까운 K개의 이웃을 선택합니다.
3. **다수결 투표(분류)**: 선택된 K개의 이웃 중 가장 많이 등장하는 클래스 레이블을 새로운 데이터 포인트의 레이블로 할당합니다.
4. **평균 계산(회귀)**: 선택된 K개의 이웃의 값을 평균 내어 새로운 데이터 포인트의 값을 예측합니다.

## 장점
- **단순함**: KNN은 이해하고 구현하기 쉬운 알고리즘입니다.
- **비모수적 방법**: KNN은 데이터의 분포에 대한 가정을 필요로 하지 않습니다.
- **다목적성**: 분류와 회귀 문제 모두에 사용할 수 있습니다.

## 단점
- **계산 비용**: 모든 데이터 포인트 간의 거리를 계산해야 하므로, 데이터가 클 경우 계산 비용이 많이 듭니다.
- **저장 공간**: 모든 학습 데이터를 저장해야 하므로, 저장 공간이 많이 필요합니다.
- **특성 스케일링**: 거리 기반 알고리즘이므로, 특성의 스케일에 민감합니다. 따라서 표준화나 정규화가 필요할 수 있습니다.

## 적용 사례
- **추천 시스템**: 사용자와 유사한 다른 사용자들의 선호도를 기반으로 추천 항목을 제공합니다.
- **이미지 인식**: 이미지의 픽셀 값을 기반으로 유사한 이미지를 분류합니다.
- **의료 진단**: 환자의 증상 데이터를 기반으로 유사한 증상을 가진 환자들의 진단 결과를 참고하여 진단을 내립니다.

## 결론
K-최근접 이웃(KNN) 알고리즘은 단순하면서도 강력한 지도 학습 방법입니다. 데이터의 분포에 대한 가정이 필요 없고, 분류와 회귀 문제 모두에 적용할 수 있어 다양한 분야에서 활용되고 있습니다. 그러나 계산 비용과 저장 공간의 한계가 있으므로, 대규모 데이터에서는 다른 알고리즘과의 조합이나 최적화가 필요할 수 있습니다.
